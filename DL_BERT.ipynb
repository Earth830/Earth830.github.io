{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e9827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# โค้ดนี้จะช่วยในการเตรียมข้อมูลสำหรับการฝึกและทดสอบโมเดลการวิเคราะห์ข้อความ โดยการแยกข้อมูลตามข้อความ ความรู้สึก และหัวข้อ พร้อมทั้งบันทึกข้อมูลที่เตรียมไว้ให้สามารถใช้งานในภายหลังได้ โดยการใช้ pickle เก็บแผนที่และข้อมูลต่าง ๆ ทั้งหมดในไฟล์ .pkl ครับ\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_excel = r\"C:\\othai\\ML_BERT\\ML_Excel\\externn.xlsx\"\n",
    "output_dir  = r\"C:\\othai\\ML_BERT\\Model\\pk2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "topic_list = [\n",
    "    \"Activity\", \"After-Service\", \"Appreciation\", \"Chinese Investors\",\n",
    "    \"Common Area - Facilities\", \"Construction Materials\", \"Design\", \"Engaging\",\n",
    "    \"Financial & Branding\", \"Intention\", \"Location\", \"Pet\",\n",
    "    \"Politics (delete after)\", \"Price & Promotion\", \"Quality\", \"Security\", \"Space\"\n",
    "]\n",
    "topic_map  = {t: i for i, t in enumerate(topic_list)}\n",
    "sentiment_map = {\"Positive\": 0, \"Neutral\": 1, \"Negative\": 2}\n",
    "\n",
    "df = pd.read_excel(input_excel)\n",
    "df = df.dropna(subset=[\"message\", \"sentiment\", \"topic\"])\n",
    "\n",
    "\n",
    "df[\"message\"] = df[\"message\"].astype(str).str.strip().str.lower()\n",
    "df = df.drop_duplicates(subset=[\"message\"], keep=\"first\")\n",
    "\n",
    "df[\"sentiment_label\"] = df[\"sentiment\"].map(sentiment_map)\n",
    "df[\"topic_label\"]     = df[\"topic\"].map(topic_map)\n",
    "\n",
    "# Check for mapping errors\n",
    "if df[\"sentiment_label\"].isnull().any():\n",
    "    raise ValueError(\"พบ sentiment ที่ map ไม่ได้:\", df[df[\"sentiment_label\"].isnull()])\n",
    "if df[\"topic_label\"].isnull().any():\n",
    "    raise ValueError(\"พบ topic ที่ map ไม่ได้:\", df[df[\"topic_label\"].isnull()])\n",
    "\n",
    "\n",
    "train_texts, test_texts, \\\n",
    "train_sentiments, test_sentiments, \\\n",
    "train_topics, test_topics = train_test_split(\n",
    "    df[\"message\"].tolist(),\n",
    "    df[\"sentiment_label\"].tolist(),\n",
    "    df[\"topic_label\"].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"sentiment_label\"]\n",
    ")\n",
    "\n",
    "# Save mapping for reuse\n",
    "with open(os.path.join(output_dir, \"topic_map.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(topic_map, f)\n",
    "with open(os.path.join(output_dir, \"sentiment_map.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(sentiment_map, f)\n",
    "\n",
    "\n",
    "paths = {\n",
    "    \"train_texts\":      os.path.join(output_dir, \"train_texts.pkl\"),\n",
    "    \"test_texts\":       os.path.join(output_dir, \"test_texts.pkl\"),\n",
    "    \"train_sentiment\":  os.path.join(output_dir, \"train_sentiment.pkl\"),\n",
    "    \"test_sentiment\":   os.path.join(output_dir, \"test_sentiment.pkl\"),\n",
    "    \"train_topic\":      os.path.join(output_dir, \"train_topic.pkl\"),\n",
    "    \"test_topic\":       os.path.join(output_dir, \"test_topic.pkl\"),\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"train_texts\":     train_texts,\n",
    "    \"test_texts\":      test_texts,\n",
    "    \"train_sentiment\": train_sentiments,\n",
    "    \"test_sentiment\":  test_sentiments,\n",
    "    \"train_topic\":     train_topics,\n",
    "    \"test_topic\":      test_topics,\n",
    "}\n",
    "\n",
    "for name, arr in data.items():\n",
    "    with open(paths[name], \"wb\") as f:\n",
    "        pickle.dump(arr, f)\n",
    "    print(f\"Saved {name} → {paths[name]}\")\n",
    "\n",
    "print(f\"Train samples: {len(train_texts)}, Test samples: {len(test_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eeb86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#โค้ดนี้ช่วยในการเตรียมข้อมูลสำหรับโมเดลภาษาโดยการแปลงข้อความเป็น tokens ซึ่งจะใช้ในกระบวนการฝึกและทดสอบโมเดล โดยทำการโหลดข้อมูลจากไฟล์ .pkl ที่มีข้อความแล้วใช้ AutoTokenizer แปลงข้อความเป็น PyTorch tensors ที่พร้อมใช้งานในขั้นตอนถัดไป จากนั้นบันทึกการเข้ารหัสเหล่านี้ไปยังไฟล์ใหม่ครับ\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#  Config Paths \n",
    "DATA_DIR          = r\"C:\\othai\\ML_BERT\\Model\\pk2\"\n",
    "DEFAULT_TRAIN     = os.path.join(DATA_DIR, \"train_texts.pkl\")\n",
    "DEFAULT_TEST      = os.path.join(DATA_DIR, \"test_texts.pkl\")\n",
    "OUTPUT_DIR        = os.path.join(DATA_DIR, \"encodings\")\n",
    "DEFAULT_TRAIN_OUT = os.path.join(OUTPUT_DIR, \"train_encodings.pkl\")\n",
    "DEFAULT_TEST_OUT  = os.path.join(OUTPUT_DIR, \"test_encodings.pkl\")\n",
    "\n",
    "\n",
    "def load_tokenizer(model_name: str, spm_model_path: str = None):\n",
    "    \"\"\"\n",
    "    Load a slow tokenizer (use_fast=False).\n",
    "    If a SentencePiece model file is given, pass it via sp_model_kwargs.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading tokenizer (slow) for: {model_name}\")\n",
    "    kwargs = {\"use_fast\": False}\n",
    "    if spm_model_path:\n",
    "        logger.info(f\"→ using SentencePiece model file: {spm_model_path}\")\n",
    "        kwargs[\"sp_model_kwargs\"] = {\"model_file\": spm_model_path}\n",
    "    return AutoTokenizer.from_pretrained(model_name, **kwargs)\n",
    "\n",
    "\n",
    "def tokenize_texts(tokenizer, texts: list[str], max_length: int):\n",
    "    \"\"\"\n",
    "    Tokenize a list of texts, returning PyTorch tensors.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Tokenizing {len(texts)} texts (max_length={max_length})\")\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "\n",
    "def save_encodings(encodings, path: str):\n",
    "    \"\"\"\n",
    "    Save PyTorch-tensor encodings to a pickle file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(encodings, f)\n",
    "    logger.info(f\"Saved encodings → {path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Step 2: Tokenize texts & save encodings\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name\",\n",
    "        default=\"airesearch/wangchanberta-base-att-spm-uncased\",\n",
    "        help=\"HuggingFace model name for tokenizer\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--spm_model\",\n",
    "        default=None,\n",
    "        help=\"Optional path to SentencePiece .model file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_length\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"Maximum token length\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_texts\",\n",
    "        default=DEFAULT_TRAIN,\n",
    "        help=\"Path to train_texts.pkl\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test_texts\",\n",
    "        default=DEFAULT_TEST,\n",
    "        help=\"Path to test_texts.pkl\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_output\",\n",
    "        default=DEFAULT_TRAIN_OUT,\n",
    "        help=\"Where to save train encodings\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test_output\",\n",
    "        default=DEFAULT_TEST_OUT,\n",
    "        help=\"Where to save test encodings\"\n",
    "    )\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    if unknown:\n",
    "        logger.warning(f\"Ignoring unknown args: {unknown}\")\n",
    "\n",
    "    tokenizer = load_tokenizer(args.model_name, args.spm_model)\n",
    "\n",
    "    for split in (\"train\", \"test\"):\n",
    "        in_path  = getattr(args, f\"{split}_texts\")\n",
    "        out_path = getattr(args, f\"{split}_output\")\n",
    "        if not os.path.exists(in_path):\n",
    "            logger.error(f\"File not found: {in_path}\")\n",
    "            continue\n",
    "        with open(in_path, \"rb\") as f:\n",
    "            texts = pickle.load(f)\n",
    "        enc = tokenize_texts(tokenizer, texts, args.max_length)\n",
    "        save_encodings(enc, out_path)\n",
    "\n",
    "    logger.info(\"complete: encodings saved.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3fb6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#โค้ดนี้ใช้เพื่อเตรียมข้อมูลและโหลดข้อมูลที่เข้ารหัสแล้ว  tokenized text ของ sentiment สำหรับการใช้ใน PyTorch โมเดล มันจัดการการโหลดข้อมูลในรูปแบบ Dataset โดยสามารถใช้ GPU ได้ และยังรองรับการประมวลผลข้อมูลแบบขนานผ่าน multiprocessing เพื่อเพิ่มประสิทธิภาพในการโหลดข้อมูลครับ\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import multiprocessing\n",
    "\n",
    "#  Setup Logger \n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_pickle(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        logger.error(f\"File not found: {path}\")\n",
    "        raise FileNotFoundError(path)\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    logger.info(f\"Loaded {len(data)} items from {path}\")\n",
    "    return data\n",
    "\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, encodings: dict, labels: list[int]):\n",
    "        n = len(next(iter(encodings.values())))\n",
    "        if len(labels) != n:\n",
    "            raise ValueError(f\"Labels count ({len(labels)}) != encodings count ({n})\")\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx: int):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "def batch_to_device(batch: dict, device: torch.device):\n",
    "    return {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    if device.type == \"cuda\":\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    ENCODINGS_DIR = r\"C:\\othai\\ML_BERT\\Model\\pk2\\encodings\"\n",
    "    DATA_DIR      = r\"C:\\othai\\ML_BERT\\Model\\pk2\"\n",
    "    train_enc_path = os.path.join(ENCODINGS_DIR, \"train_encodings.pkl\")\n",
    "    test_enc_path  = os.path.join(ENCODINGS_DIR, \"test_encodings.pkl\")\n",
    "    train_lbl_path = os.path.join(DATA_DIR, \"train_sentiment.pkl\")\n",
    "    test_lbl_path  = os.path.join(DATA_DIR, \"test_sentiment.pkl\")\n",
    "\n",
    "    train_enc = load_pickle(train_enc_path)\n",
    "    test_enc  = load_pickle(test_enc_path)\n",
    "    train_lbl = load_pickle(train_lbl_path)\n",
    "    test_lbl  = load_pickle(test_lbl_path)\n",
    "\n",
    "    batch_size = 8\n",
    "    pin_memory = device.type == \"cuda\"\n",
    "    num_workers = 0 if os.name == 'nt' else max(1, os.cpu_count() // 2)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        CommentDataset(train_enc, train_lbl),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        CommentDataset(test_enc, test_lbl),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Train set: {len(train_loader.dataset)} samples, {len(train_loader)} batches (workers={num_workers})\")\n",
    "    logger.info(f\"Test  set: {len(test_loader.dataset)} samples, {len(test_loader)} batches\")\n",
    "\n",
    "    try:\n",
    "        batch = next(iter(train_loader))\n",
    "        logger.info({k: v.shape for k, v in batch.items()})\n",
    "        batch = batch_to_device(batch, device)\n",
    "        logger.info(f\"Batch moved to device: {{k: v.device for k, v in batch.items()}}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load a batch: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24857020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#โค้ดนี้ใช้เพื่อเตรียมข้อมูลและโหลดข้อมูลที่เข้ารหัสแล้ว  tokenized text ของ topic สำหรับการใช้ใน PyTorch โมเดล มันจัดการการโหลดข้อมูลในรูปแบบ Dataset โดยสามารถใช้ GPU ได้ และยังรองรับการประมวลผลข้อมูลแบบขนานผ่าน multiprocessing เพื่อเพิ่มประสิทธิภาพในการโหลดข้อมูลครับ\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import multiprocessing\n",
    "\n",
    "#  Setup Logger \n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_pickle(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        logger.error(f\"File not found: {path}\")\n",
    "        raise FileNotFoundError(path)\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    logger.info(f\"Loaded {len(data)} items from {path}\")\n",
    "    return data\n",
    "\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, encodings: dict, labels: list[int]):\n",
    "        n = len(next(iter(encodings.values())))\n",
    "        if len(labels) != n:\n",
    "            raise ValueError(f\"Labels count ({len(labels)}) != encodings count ({n})\")\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx: int):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "def batch_to_device(batch: dict, device: torch.device):\n",
    "    return {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    if device.type == \"cuda\":\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    ENCODINGS_DIR = r\"C:\\othai\\ML_BERT\\Model\\pk2\\encodings\"\n",
    "    DATA_DIR      = r\"C:\\othai\\ML_BERT\\Model\\pk2\"\n",
    "    train_enc_path = os.path.join(ENCODINGS_DIR, \"train_encodings.pkl\")\n",
    "    test_enc_path  = os.path.join(ENCODINGS_DIR, \"test_encodings.pkl\")\n",
    "    train_lbl_path = os.path.join(DATA_DIR, \"train_topic.pkl\")\n",
    "    test_lbl_path  = os.path.join(DATA_DIR, \"test_topic.pkl\")\n",
    "\n",
    "    train_enc = load_pickle(train_enc_path)\n",
    "    test_enc  = load_pickle(test_enc_path)\n",
    "    train_lbl = load_pickle(train_lbl_path)\n",
    "    test_lbl  = load_pickle(test_lbl_path)\n",
    "\n",
    "    batch_size = 8\n",
    "    pin_memory = device.type == \"cuda\"\n",
    "    num_workers = 0 if os.name == 'nt' else max(1, os.cpu_count() // 2)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        CommentDataset(train_enc, train_lbl),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        CommentDataset(test_enc, test_lbl),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Train set: {len(train_loader.dataset)} samples, {len(train_loader)} batches (workers={num_workers})\")\n",
    "    logger.info(f\"Test  set: {len(test_loader.dataset)} samples, {len(test_loader)} batches\")\n",
    "\n",
    "    try:\n",
    "        batch = next(iter(train_loader))\n",
    "        logger.info({k: v.shape for k, v in batch.items()})\n",
    "        batch = batch_to_device(batch, device)\n",
    "        logger.info(f\"Batch moved to device: {{k: v.device for k, v in batch.items()}}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load a batch: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8581645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#โต้ดนี้ใช้เพื่อฝึกโมเดลการจำแนกประเภทข้อความ โดยเป็นส่วนการฝึกโมเดลให้เรียนรู้ว่าเจอความคิดเห็นแบบไหนเป็นแง่บวก แง่ลบ หรือเป็นกลาง\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_pickle(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        logger.error(f\"File not found: {path}\")\n",
    "        raise FileNotFoundError(path)\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    logger.info(f\"Loaded {len(data)} items from {path}\")\n",
    "    return data\n",
    "\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, encodings: dict, labels: list[int]):\n",
    "        n = len(next(iter(encodings.values())))\n",
    "        if len(labels) != n:\n",
    "            raise ValueError(f\"Labels count ({len(labels)}) != encodings count ({n})\")\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx: int):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds  = pred.predictions.argmax(-1)\n",
    "    acc    = accuracy_score(labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    if device.type == \"cuda\":\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    base_dir   = r\"C:\\othai\\ML_BERT\\Model\\pk2\"\n",
    "    enc_dir    = os.path.join(base_dir, \"encodings\")\n",
    "    output_dir = r\"C:\\othai\\ML_BERT\\Model\\model_sentiment\"\n",
    "    model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "    num_labels = 3\n",
    "\n",
    "    train_enc = load_pickle(os.path.join(enc_dir, \"train_encodings.pkl\"))\n",
    "    test_enc  = load_pickle(os.path.join(enc_dir, \"test_encodings.pkl\"))\n",
    "    train_lbl = load_pickle(os.path.join(base_dir, \"train_sentiment.pkl\"))\n",
    "    test_lbl  = load_pickle(os.path.join(base_dir, \"test_sentiment.pkl\"))\n",
    "\n",
    "    train_dataset = CommentDataset(train_enc, train_lbl)\n",
    "    eval_dataset  = CommentDataset(test_enc,  test_lbl)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=num_labels\n",
    "    ).to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=6,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.02,\n",
    "        warmup_steps=int((len(train_dataset)//16)*6*0.1),\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=0 if os.name=='nt' else max(1, os.cpu_count()//2),\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    logger.info(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    logger.info(\"Evaluating on test set...\")\n",
    "    metrics = trainer.evaluate()\n",
    "    logger.info({k: f\"{v:.4f}\" for k, v in metrics.items()})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56647259",
   "metadata": {},
   "outputs": [],
   "source": [
    "##โต้ดนี้ใช้เพื่อฝึกโมเดลการจำแนกประเภทข้อความ โดยเป็นส่วนการฝึกโมเดลให้เรียนรู้ว่าเพื่อจำแนก topic จากความคิดเห็นที่มีอยู่ โดยใช้โมเดล BERT ที่ถูกฝึกมาแล้ว\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch import nn\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_pickle(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        logger.error(f\"File not found: {path}\")\n",
    "        raise FileNotFoundError(path)\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    logger.info(f\"Loaded {len(data)} items from {path}\")\n",
    "    return data\n",
    "\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, encodings: dict, labels: list[int]):\n",
    "        n = len(next(iter(encodings.values())))\n",
    "        if len(labels) != n:\n",
    "            raise ValueError(f\"Labels count ({len(labels)}) != encodings count ({n})\")\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx: int):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds  = pred.predictions.argmax(-1)\n",
    "    acc    = accuracy_score(labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, loss_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_weights = loss_weights\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=self.loss_weights.to(labels.device) if self.loss_weights is not None else None)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    if device.type == \"cuda\":\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    base_dir   = r\"C:\\othai\\ML_BERT\\Model\\pk2\"\n",
    "    enc_dir    = os.path.join(base_dir, \"encodings\")\n",
    "    output_dir = r\"C:\\othai\\ML_BERT\\Model\\model1\"\n",
    "    model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "    num_labels = 17\n",
    "\n",
    "    train_enc = load_pickle(os.path.join(enc_dir, \"train_encodings.pkl\"))\n",
    "    test_enc  = load_pickle(os.path.join(enc_dir, \"test_encodings.pkl\"))\n",
    "    train_lbl = load_pickle(os.path.join(base_dir, \"train_topic.pkl\"))\n",
    "    test_lbl  = load_pickle(os.path.join(base_dir, \"test_topic.pkl\"))\n",
    "\n",
    "    train_dataset = CommentDataset(train_enc, train_lbl)\n",
    "    eval_dataset  = CommentDataset(test_enc,  test_lbl)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=num_labels\n",
    "    ).to(device)\n",
    "\n",
    "    # Weighted loss\n",
    "    label_counts = Counter(train_lbl)\n",
    "    n_classes = 17\n",
    "    weights = np.zeros(n_classes, dtype=np.float32)\n",
    "    for i in range(n_classes):\n",
    "        weights[i] = 1.0 / (label_counts[i] if label_counts[i] > 0 else 1)\n",
    "    weights = weights * (len(train_lbl) / np.sum(weights))\n",
    "    loss_weights = torch.tensor(weights, dtype=torch.float32)\n",
    "    logger.info(f\"Topic class weights: {loss_weights}\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=20,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=1e-5,\n",
    "        weight_decay=0.02,\n",
    "        warmup_steps=int((len(train_dataset)//16)*10*0.1),\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=0 if os.name=='nt' else max(1, os.cpu_count()//2),\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "        loss_weights=loss_weights\n",
    "    )\n",
    "\n",
    "    logger.info(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    logger.info(\"Evaluating on test set...\")\n",
    "    metrics = trainer.evaluate()\n",
    "    logger.info({k: f\"{v:.4f}\" for k, v in metrics.items()})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368633cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#โค้ดนี้ใช้เพื่อประมวลผลข้อความจากไฟล์ Excel หลายแผ่น โดยการใช้โมเดล BERT ที่ถูกฝึกมาแล้วเพื่อจำแนกความคิดเห็นเป็นแง่บวก แง่ลบ หรือเป็นกลาง และจำแนกหัวข้อที่เกี่ยวข้องกับความคิดเห็นนั้น ๆ จากนั้นจะบันทึกผลลัพธ์ลงในไฟล์ Excel ใหม่เพื่อให้สามารถใช้งานต่อได้ในอนาคตครับ\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "def get_preds(model, tokenizer, texts, device, batch_size=32, max_length=128, label_map=None):\n",
    "    dataset = TextDataset(texts)\n",
    "    def collate_fn(batch):\n",
    "        return tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length\n",
    "        )\n",
    "    loader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=0 if os.name == 'nt' else os.cpu_count(),\n",
    "        pin_memory=(device.type == \"cuda\"), collate_fn=collate_fn\n",
    "    )\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    for batch in tqdm(loader, desc=\"Predicting\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**batch).logits\n",
    "        pred = logits.argmax(dim=-1).cpu().tolist()\n",
    "        if label_map is not None:\n",
    "            pred = [label_map.get(p, str(p)) for p in pred]\n",
    "        preds.extend(pred)\n",
    "    return preds\n",
    "\n",
    "def main():\n",
    "    #  SET PATH \n",
    "    sentiment_ckpt = r\"C:\\othai\\ML_BERT\\Model\\model_sentiment\\checkpoint-1000\"\n",
    "    topic_ckpt     = r\"C:\\othai\\ML_BERT\\Model\\model_topic\\checkpoint-1200\"\n",
    "    input_excel    = r\"C:\\othai\\ML_BERT\\ML_Excel\\extern11.xlsx\"\n",
    "    output_excel   = r\"C:\\othai\\ML_BERT\\ML_Excel\\result_dual1.xlsx\"\n",
    "\n",
    "    sentiment_map = {0: \"Positive\", 1: \"Neutral\", 2: \"Negative\"}\n",
    "    topic_map = {\n",
    "        0:\"Activity\", 1:\"After-Service\", 2:\"Appreciation\", 3:\"Chinese Investors\",\n",
    "        4:\"Common Area - Facilities\", 5:\"Construction Materials\", 6:\"Design\",\n",
    "        7:\"Engaging\", 8:\"Financial & Branding\", 9:\"Intention\", 10:\"Location\",\n",
    "        11:\"Pet\", 12:\"Politics (delete after)\", 13:\"Price & Promotion\", 14:\"Quality\",\n",
    "        15:\"Security\", 16:\"Space\"\n",
    "    }\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_ckpt)\n",
    "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_ckpt).to(device)\n",
    "    topic_tokenizer = AutoTokenizer.from_pretrained(topic_ckpt)\n",
    "    topic_model = AutoModelForSequenceClassification.from_pretrained(topic_ckpt).to(device)\n",
    "\n",
    "    # --- LOAD ALL SHEETS ---\n",
    "    all_sheets = pd.read_excel(input_excel, sheet_name=None)\n",
    "    output_sheets = {}\n",
    "\n",
    "    for sheet_name, df in all_sheets.items():\n",
    "        if \"message\" not in df.columns:\n",
    "            print(f\"⚠️  Sheet {sheet_name} ไม่มีคอลัมน์ 'message' ข้าม sheet นี้\")\n",
    "            continue\n",
    "        df[\"message\"] = df[\"message\"].fillna(\"\").astype(str)\n",
    "        texts = df[\"message\"].tolist()\n",
    "        sent_preds = get_preds(sentiment_model, sentiment_tokenizer, texts, device, label_map=sentiment_map)\n",
    "        topic_preds = get_preds(topic_model, topic_tokenizer, texts, device, label_map=topic_map)\n",
    "        df[\"sentiment_pred\"] = sent_preds\n",
    "        df[\"topic_pred\"] = topic_preds\n",
    "        output_sheets[sheet_name] = df\n",
    "\n",
    "    # --- EXPORT ---\n",
    "    with pd.ExcelWriter(output_excel) as writer:\n",
    "        for sheet_name, df in output_sheets.items():\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    print(f\"Exported all sheets to {output_excel}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
